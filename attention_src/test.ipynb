{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "Namespace(attention_size=384, batch_size=1, caption_path='../mscoco/annotations/captions_train2014.json', decoder_path='./coco_models/decoder-9-2232.ckpt', embed_size=256, encoded_image_size=14, encoder_path='./coco_models/encoder-9-2232.ckpt', hidden_size=384, image_dir='../mscoco/resized_train2014', image_size=224, num_workers=4, vocab_path='./data/coco_vocab.pkl')\n",
      "loading annotations into memory...\n",
      "Done (t=0.79s)\n",
      "creating index...\n",
      "index created!\n",
      "0.6111111111111112\n",
      "0.4194352464039306\n",
      "0.24474633652528716\n",
      "0.16140750043829394\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from coco_data_loader import get_validation_loader \n",
    "from coco_build_vocab import Vocabulary\n",
    "from coco_model import EncoderCNN, DecoderRNNWithAttention\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import transforms\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "def main(args):\n",
    "    # Image preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((args.image_size, args.image_size)),\n",
    "        transforms.ToTensor(), \n",
    "     #   transforms.Normalize((0.485, 0.456, 0.406), \n",
    "      #                       (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    \n",
    "    # Load vocabulary wrapper\n",
    "    with open(args.vocab_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        \n",
    "    # Build data loader\n",
    "    data_loader = get_validation_loader(args.image_dir, args.caption_path, vocab, \n",
    "                             transform, args.batch_size,\n",
    "                             num_workers=args.num_workers)\n",
    "    \n",
    "    # Build models\n",
    "    encoder = EncoderCNN(args.encoded_image_size).eval()  # eval mode (batchnorm uses moving mean/variance)\n",
    "    decoder = DecoderRNNWithAttention(args.embed_size, args.attention_size, args.hidden_size, len(vocab)).eval()\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "\n",
    "    # Load the trained model parameters\n",
    "    encoder.load_state_dict(torch.load(args.encoder_path))\n",
    "    decoder.load_state_dict(torch.load(args.decoder_path))\n",
    "    \n",
    "    ground_truth = []\n",
    "    predicted = []\n",
    "    for i, (images, captions) in enumerate(data_loader):\n",
    "        # Set mini-batch dataset\n",
    "        images = images.to(device)\n",
    "        features = encoder(images)\n",
    "        sampled_seq = decoder.sample_beam_search(features, vocab, device)\n",
    "        ground_truth.append(captions[0])\n",
    "        predicted.append(sampled_seq[0])\n",
    "        if i > 10: break\n",
    "            \n",
    "    print(corpus_bleu(ground_truth, predicted, weights=(1, 0, 0, 0)))\n",
    "    print(corpus_bleu(ground_truth, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print(corpus_bleu(ground_truth, predicted, weights=(1.0/3.0, 1.0/3.0, 1.0/3.0, 0)))\n",
    "    print(corpus_bleu(ground_truth, predicted))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    sys.argv = ['foo']\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--encoder_path', type=str, default='./coco_models/encoder-9-2232.ckpt', help='path for trained encoder')\n",
    "    parser.add_argument('--decoder_path', type=str, default='./coco_models/decoder-9-2232.ckpt', help='path for trained decoder')\n",
    "    parser.add_argument('--vocab_path', type=str, default='./data/coco_vocab.pkl', help='path for vocabulary wrapper')\n",
    "    parser.add_argument('--image_dir', type=str, default='../mscoco/resized_train2014', help='directory for resized images')\n",
    "    parser.add_argument('--caption_path', type=str, default='../mscoco/annotations/captions_train2014.json', help='path for train annotation json file')\n",
    "    parser.add_argument('--image_size', type=int , default=224, help='input image size')\n",
    "\n",
    "    # Model parameters (should be same as paramters in train.py)\n",
    "    parser.add_argument('--embed_size', type=int , default=256, help='dimension of word embedding vectors')\n",
    "    parser.add_argument('--encoded_image_size', type=int , default=14, help='dimension of encoded image')\n",
    "    parser.add_argument('--attention_size', type=int , default=384, help='dimension of attention layers')\n",
    "    parser.add_argument('--hidden_size', type=int , default=384, help='dimension of lstm hidden states')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=1)\n",
    "    parser.add_argument('--num_workers', type=int, default=4)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
